{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# This script upload local files to a blob in the azure cloud.\n",
    "# It needs account information:\n",
    "#   - Account name.\n",
    "#   - Account key.\n",
    "# It needs the blob container information\n",
    "#   - Container name\n",
    "#   - Container sub-directory\n",
    "#------------------------------------------------------------------------------\n",
    "import os, uuid, sys\n",
    "import subprocess\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import astropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from io import BytesIO\n",
    "from astropy.io import fits\n",
    "from functools import partial\n",
    "from azure.storage.blob import BlockBlobService, PublicAccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_blob_keys as cfg\n",
    "#!pip install azure-storage-blob\n",
    "# Create the BlockBlockService that is used to call the Blob service \n",
    "# for the storage account\n",
    "account_name = cfg.AccountName\n",
    "account_key = cfg.AccountKey\n",
    "block_blob_service = BlockBlobService(account_name=account_name, account_key=account_key)\n",
    "\n",
    "cont_name_unc = cfg.ContNameUnC\n",
    "block_blob_service.set_container_acl(cont_name_unc, public_access=PublicAccess.Container)\n",
    "\n",
    "cont_name_proc = cfg.ContNameProc\n",
    "block_blob_service.set_container_acl(cont_name_proc, public_access=PublicAccess.Container)\n",
    "\n",
    "# Number of workers\n",
    "NumberWorkers=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list \"filelist\" with the blob content\n",
    "# inside the \"Azure:container/folder\" location \n",
    "def BlobList(container, folder, filelist, verbose=False):\n",
    "    \n",
    "    gen = block_blob_service.list_blobs(container, prefix=folder)\n",
    "    \n",
    "    for blob in gen:\n",
    "        file = str(blob.name).replace(folder,'')\n",
    "        filelist.append(file)\n",
    "        if verbose == True:\n",
    "            print(\"\\t Blob name: \" + blob.name)\n",
    "        \n",
    "    return filelist\n",
    "\n",
    "# Download a file \"blobfile\" from \"container\" and save it \n",
    "# in the file \"locfile\"\n",
    "def DownBlob(container, blobfile, locfile, verbose=False):\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('Downloading ' + blobfile + ' to ' + locfile)\n",
    "    \n",
    "    block_blob_service.get_blob_to_path(container, blobfile, locfile)\n",
    "\n",
    "# Uncompress data \n",
    "def UnCompress(file, verbose=False):\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('Uncompressing ' + file)\n",
    "    \n",
    "    subprocess.call(['uncompress', file])\n",
    "    #os.popen('uncompress ' + file)\n",
    "\n",
    "# Upload file \"locfile\" to the blob \"blobfile\" in container\n",
    "def UpBlob(container, blobfile, locfile, verbose=False):\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('Uploading ' + locfile + ' to ' + blobfile)\n",
    "        \n",
    "    block_blob_service.create_blob_from_path(container, blobfile, locfile, validate_content=True)\n",
    "    \n",
    "# Decompose .fits files  from uncompressed in numpy arrays containing hdu.data data\n",
    "# and upload the data to processed/numpy\n",
    "def TransformFits(path_loc, unc_blob_sub_dir, npy_blob_sub_dir, file, verbose=False, format='numpy'):       \n",
    "    # Download the data from uncompressed\n",
    "    unc_blob_name = os.path.join(unc_blob_sub_dir,file)\n",
    "    path_to_file_loc = os.path.join(path_loc, file)\n",
    "\n",
    "    DownBlob(cont_name_unc, unc_blob_name, path_to_file_loc, False)\n",
    "    \n",
    "    while not os.path.exists(path_to_file_loc):\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Extract data from fits\n",
    "    if format=='numpy':\n",
    "        format_dir='numpy'\n",
    "\n",
    "        hdu_list = fits.open(path_to_file_loc)\n",
    "        \n",
    "        orig_file_name = hdu_list[0].header['ORIGFILE']\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('File name = ' + orig_file_name)  \n",
    "            \n",
    "        npy_folder = ''.join([npy_blob_sub_dir[i] for i in range(len(npy_blob_sub_dir)) if npy_blob_sub_dir[i] in orig_file_name])\n",
    "        \n",
    "        try:\n",
    "            for ext, hdu in enumerate(hdu_list):\n",
    "                if verbose == True:\n",
    "                    print(\"\\nProcessing index: \", ext)\n",
    "\n",
    "                #print((hdu.header)[0:100])\n",
    "                try:\n",
    "                    header_naxis=hdu.header['NAXIS']\n",
    "                except:\n",
    "                    header_naxis=None\n",
    "\n",
    "                #if header_value != None:\n",
    "                try:\n",
    "                    data_array = np.array(hdu.data)\n",
    "                except:\n",
    "                    data_array = np.array(0)\n",
    "\n",
    "                #print(\"Extension: \", ext, \" - Data shape: \", data_array.shape)\n",
    "                if header_naxis==2 & len(data_array.shape)==2:\n",
    "                    if verbose == True:\n",
    "                        print(\"\\nProcessing index: \", ext)\n",
    "                    loc_np_file = path_to_file_loc.replace('.fits','')\n",
    "                    loc_np_file = loc_np_file + '_ext' + str(ext) + '.npy'\n",
    "                    #data_array = np.array(hdu.data)\n",
    "\n",
    "                    if verbose == True:\n",
    "                        print('\\nSaving: ' + loc_np_file)\n",
    "                    np.save(loc_np_file, data_array)\n",
    "\n",
    "                    # Upload the data\n",
    "                    while not os.path.exists(loc_np_file):\n",
    "                        time.sleep(0.1)\n",
    "\n",
    "                    if verbose == True:\n",
    "                        statinfo=os.stat(loc_np_file)\n",
    "                        print(\"File size {} MB\".format(statinfo.st_size/1024**2))\n",
    "\n",
    "                    np_file = file.replace('.fits','.npy')#orig_file_name.replace('.fits','')\n",
    "                    #np_file = np_file + '_ext' + str(ext) + '.npy'\n",
    "                    #np_file = np_file + '.npy'\n",
    "                    ext_folder = 'ext' + str(ext)\n",
    "                    npy_blob_name = os.path.join(format_dir, os.path.join(npy_folder, ext_folder))\n",
    "                    npy_blob_name = os.path.join(npy_blob_name, np_file)\n",
    "                    #print('Blob name = ' + npy_blob_name)\n",
    "                    #print('Loc name  = ' + loc_np_file)\n",
    "                    UpBlob(cont_name_proc, npy_blob_name, loc_np_file, False)\n",
    "\n",
    "                    # Remove uploaded array\n",
    "                    os.remove(loc_np_file)\n",
    "                elif header_naxis==2:\n",
    "                    print(\"Corrupted file - blob_name: \", unc_blob_name)\n",
    "                    with open('transform_fits_corrupted.lst', 'a') as fd:\n",
    "                        fd.write(unc_blob_name +'\\n')\n",
    "\n",
    "            hdu_list.close()\n",
    "        except:\n",
    "            print(\"Check file - blob_name: \", unc_blob_name)\n",
    "            with open('transform_fits_check.lst', 'a') as fd:\n",
    "                fd.write(unc_blob_name +'\\n')\n",
    "            \n",
    "                \n",
    "    elif format=='parquet':\n",
    "        format_dir='parquet'\n",
    "\n",
    "        hdu_list = fits.open(path_to_file_loc)\n",
    "        for ext, hdu in enumerate(hdu_list):\n",
    "            print(\"\\nProcessing index: \", ext)\n",
    "            try:\n",
    "                header_value=hdu.header['XTENSION']\n",
    "            except:\n",
    "                header_value=None\n",
    "        hdu_list.close()\n",
    "        \n",
    "    # Remove .fits        \n",
    "    os.remove(path_to_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on bias_blue...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649c1a90d2db41f0b22365ebadb21677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16227), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dir time:  891.9518530368805\n",
      "Working on bias_red...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a198c5bcb5af4fecbc63207cf3191a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16591), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dir time:  2271.333835840225\n",
      "Working on blue_arc_flat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e218df483447aeb2a2d83b96d910bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9861), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dir time:  798.6886813640594\n",
      "Working on red_arc_flat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0d7ef2a7324bd18a317b54da1c7c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6865), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dir time:  1081.1292214393616\n",
      "Total out time:  5080.136281490326\n"
     ]
    }
   ],
   "source": [
    "# Script to dowload the uncompressed data and transform the images \n",
    "# into numpy arrays for each image extension \n",
    "\n",
    "# Flags for checks\n",
    "check_from_file = False\n",
    "check_from_list = False\n",
    "\n",
    "uncBlobSubDirs = ['bias_blue', 'bias_red', 'blue_arc_flat','red_arc_flat']\n",
    "#Test\n",
    "#uncBlobSubDirs = ['red_arc_flat']\n",
    "path_loc = '../Temp'\n",
    "\n",
    "start_time_out = time.time()\n",
    "\n",
    "# loof for uncompressed folders\n",
    "for unc_blob_sub_dir in uncBlobSubDirs:\n",
    "    \n",
    "    # Define the subdirs to be created in /numpy based on the type of images\n",
    "    if unc_blob_sub_dir == 'bias_blue':\n",
    "        npy_blob_sub_dir = ['UVES_BLUE_BIAS']\n",
    "    elif unc_blob_sub_dir == 'bias_red':\n",
    "        npy_blob_sub_dir = ['UVES_RED_BIAS']\n",
    "    elif unc_blob_sub_dir == 'blue_arc_flat':\n",
    "        npy_blob_sub_dir = ['UVES_BLUE_WAVE','UVES_DIC1B_FLAT','UVES_DIC1B_DFLAT']\n",
    "    elif unc_blob_sub_dir == 'red_arc_flat':\n",
    "        npy_blob_sub_dir = ['UVES_RED_WAVE','UVES_DIC1R_FLAT']\n",
    "    \n",
    "    # List the uncompressed data\n",
    "    unc_files_list = []\n",
    "    unc_folder_rem = unc_blob_sub_dir + '/'\n",
    "\n",
    "    BlobList(cont_name_unc, unc_folder_rem, unc_files_list)\n",
    "    #unc_files_list = unc_files_list[0:1]\n",
    "\n",
    "    #if check_from_list:\n",
    "    #    fits_files = []\n",
    "    #    BlobList(container_name_fits, folder_rem, fits_files)\n",
    "    #    fits_files = [file.replace('.fits','.fits.Z') for file in fits_files]\n",
    "    #    Z_files = [file for file in Z_files if file not in fits_files]\n",
    "\n",
    "    if check_from_file:\n",
    "        corr_file = open('./transform_fits_all_check.txt','r')\n",
    "        corr_files = corr_file.readlines()\n",
    "        corr_files = [file.replace('\\n','') for file in corr_files]\n",
    "        for file in corr_files:\n",
    "            corr_list = file.split('/')\n",
    "            if corr_list[0] == unc_blob_sub_dir:\n",
    "                unc_files_list.append(corr_list[1])\n",
    "    \n",
    "    #Test\n",
    "    #unc_files_list= unc_files_list[0:10]\n",
    "    \n",
    "    start_time_dir= time.time()\n",
    "\n",
    "    if unc_files_list:\n",
    "        print('Working on ' + unc_blob_sub_dir + '...')\n",
    "\n",
    "        tasks = partial(TransformFits, path_loc, unc_blob_sub_dir, npy_blob_sub_dir)\n",
    "        with multiprocessing.Pool(NumberWorkers) as p:\n",
    "            result = list(tqdm.tqdm_notebook(p.imap(tasks, unc_files_list), total=len(unc_files_list)))\n",
    "\n",
    "    end_time_dir = time.time()\n",
    "    total_time_dir = end_time_dir - start_time_dir\n",
    "    print('Total dir time: ', total_time_dir)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time_out = end_time - start_time_out\n",
    "print('Total out time: ', total_time_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PoC ESO",
   "language": "python",
   "name": "eso"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
